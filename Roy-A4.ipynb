{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Roy-A4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNNheFGkuECv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import copy\n",
        "import sys\n",
        "\n",
        "class NeuralNetwork_Convolutional():\n",
        "    \n",
        "    def __init__(self, n_channels_in_image, image_size,\n",
        "                 n_units_in_conv_layers, kernels_size_and_stride,\n",
        "                 n_units_in_fc_hidden_layers,\n",
        "                 classes, use_gpu=False):\n",
        "\n",
        "        if not isinstance(n_units_in_conv_layers, list):\n",
        "            raise Exception('n_units_in_conv_layers must be a list')\n",
        "\n",
        "        if not isinstance(n_units_in_fc_hidden_layers, list):\n",
        "            raise Exception('n_units_in_fc_hidden_layers must be a list')\n",
        "        \n",
        "        if use_gpu and not torch.cuda.is_available():\n",
        "            print('\\nGPU is not available. Running on CPU.\\n')\n",
        "            use_gpu = False\n",
        "\n",
        "        self.n_channels_in_image = n_channels_in_image\n",
        "        self.image_size = image_size \n",
        "        self.n_units_in_conv_layers = n_units_in_conv_layers\n",
        "        self.n_units_in_fc_hidden_layers = n_units_in_fc_hidden_layers\n",
        "        self.kernels_size_and_stride = kernels_size_and_stride\n",
        "        self.n_outputs = len(classes)\n",
        "        self.classes = np.array(classes)\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        self.n_conv_layers = len(self.n_units_in_conv_layers)\n",
        "        self.n_fc_hidden_layers = len(self.n_units_in_fc_hidden_layers)\n",
        "\n",
        "        # Build the net layers\n",
        "        self.nnet = torch.nn.Sequential()\n",
        "        self.training_time=0\n",
        "        # Add convolutional layers\n",
        "\n",
        "        n_units_previous = self.n_channels_in_image\n",
        "        output_size_previous = self.image_size\n",
        "        n_layers = 0\n",
        "        if self.n_conv_layers > 0:\n",
        "\n",
        "            for (n_units, kernel) in zip(self.n_units_in_conv_layers, self.kernels_size_and_stride):\n",
        "                n_units_previous, output_size_previous = self._add_conv2d_tanh(n_layers,\n",
        "                                        n_units_previous, output_size_previous, n_units, kernel)\n",
        "                n_layers += 1 # for text label in layer\n",
        "                \n",
        "        # A4.3 version moved following statement left one indent level\n",
        "        \n",
        "        self.nnet.add_module('flatten', torch.nn.Flatten())  # prepare for fc layers\n",
        "\n",
        "        n_inputs = output_size_previous ** 2 * n_units_previous\n",
        "        if self.n_fc_hidden_layers > 0:\n",
        "            for n_units in self.n_units_in_fc_hidden_layers:\n",
        "                n_inputs = self._add_fc_tanh(n_layers, n_inputs, n_units)\n",
        "                n_layers += 1\n",
        "\n",
        "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Linear(n_inputs, self.n_outputs))\n",
        "\n",
        "        # Member variables for standardization\n",
        "        self.Xmeans = None\n",
        "        self.Xstds = None\n",
        "\n",
        "        if self.use_gpu:\n",
        "            self.nnet.cuda()\n",
        "\n",
        "        self.n_epochs = 0\n",
        "        self.error_trace = []\n",
        "\n",
        "    def _add_conv2d_tanh(self, n_layers, n_units_previous, output_size_previous,\n",
        "                   n_units, kernel_size_and_stride):\n",
        "        kernel_size, kernel_stride = kernel_size_and_stride\n",
        "        self.nnet.add_module(f'conv_{n_layers}', torch.nn.Conv2d(n_units_previous, n_units,\n",
        "                                                                 kernel_size, kernel_stride))\n",
        "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Tanh())\n",
        "        output_size_previous = (output_size_previous - kernel_size) // kernel_stride + 1\n",
        "        n_units_previous = n_units                \n",
        "        return n_units_previous, output_size_previous\n",
        "    \n",
        "    def _add_fc_tanh(self, n_layers, n_inputs, n_units):\n",
        "        self.nnet.add_module(f'linear_{n_layers}', torch.nn.Linear(n_inputs, n_units))\n",
        "        self.nnet.add_module(f'output_{n_layers}', torch.nn.Tanh())\n",
        "        n_inputs = n_units\n",
        "        return n_inputs\n",
        "\n",
        "    def __repr__(self):\n",
        "        str = f'''{type(self).__name__}(\n",
        "                            n_channels_in_image={self.n_channels_in_image},\n",
        "                            image_size={self.image_size},\n",
        "                            n_units_in_conv_layers={self.n_units_in_conv_layers},\n",
        "                            kernels_size_and_stride={self.kernels_size_and_stride},\n",
        "                            n_units_in_fc_hidden_layers={self.n_units_in_fc_hidden_layers},\n",
        "                            classes={self.classes},\n",
        "                            use_gpu={self.use_gpu})'''\n",
        "\n",
        "        str += self.nnet\n",
        "        if self.n_epochs > 0:\n",
        "            str += f'\\n   Network was trained for {self.n_epochs} epochs that took {self.training_time:.4f} seconds.'\n",
        "            str += f'\\n   Final objective value is {self.error_trace[-1]:.3f}'\n",
        "        else:\n",
        "            str += '  Network is not trained.'\n",
        "        return str\n",
        "        \n",
        "    def _standardizeX(self, X):\n",
        "        result = (X - self.Xmeans) / self.XstdsFixed\n",
        "        result[:, self.Xconstant] = 0.0\n",
        "        return result\n",
        "\n",
        "    def _unstandardizeX(self, Xs):\n",
        "        return self.Xstds * Xs + self.Xmeans\n",
        "\n",
        "    def _setup_standardize(self, X, T):\n",
        "        if self.Xmeans is None:\n",
        "            self.Xmeans = X.mean(axis=0)\n",
        "            self.Xstds = X.std(axis=0)\n",
        "            self.Xconstant = self.Xstds == 0\n",
        "            self.XstdsFixed = copy.copy(self.Xstds)\n",
        "            self.XstdsFixed[self.Xconstant] = 1\n",
        "\n",
        "    def train(self, X, T, n_epochs, learning_rate=0.01):\n",
        "\n",
        "        start_time = time.time()\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        if T.ndim == 1:\n",
        "            T = T.reshape((-1, 1))\n",
        "\n",
        "        _, T = np.where(T == self.classes)  # convert to labels from 0\n",
        "\n",
        "        self._setup_standardize(X, T)\n",
        "        X = self._standardizeX(X)\n",
        "\n",
        "        X = torch.tensor(X)\n",
        "        T = torch.tensor(T.reshape(-1))\n",
        "        if self.use_gpu:\n",
        "            X = X.cuda()\n",
        "            T = T.cuda()\n",
        "\n",
        "            \n",
        "        #    You fill in the rest of the train function, following lecture notes example.\n",
        "        #    .\n",
        "        #  \n",
        "        optimizer = torch.optim.Adam(self.nnet.parameters(), lr=learning_rate)\n",
        "        loss_F = torch.nn.CrossEntropyLoss()\n",
        "        for epoch in range(n_epochs):\n",
        "    \n",
        "          optimizer.zero_grad()\n",
        "        \n",
        "          Y = self.nnet(X)\n",
        "    \n",
        "          error = loss_F(Y, T)\n",
        "          if epoch % 50 == 0:\n",
        "              print(f'Epoch {epoch} error {error}')\n",
        "                      \n",
        "          self.error_trace.append(error)\n",
        "          error.backward()\n",
        "          \n",
        "          optimizer.step()\n",
        "        stop_time=time.time()\n",
        "        self.training_time=stop_time-start_time\n",
        "        \n",
        "    def get_error_trace(self):\n",
        "        return self.error_trace\n",
        "    \n",
        "    def _softmax(self, Y):\n",
        "        mx = Y.max()\n",
        "        expY = np.exp(Y - mx)\n",
        "        denom = expY.sum(axis=1).reshape((-1, 1)) + sys.float_info.epsilon\n",
        "        return expY / denom\n",
        "    \n",
        "    def use(self, X):\n",
        "        self.nnet.eval()  # turn off gradients and other aspects of training\n",
        "        X = self._standardizeX(X)\n",
        "        X = torch.tensor(X)\n",
        "        if self.use_gpu:\n",
        "            X = X.cuda()\n",
        "\n",
        "        Y = self.nnet(X)\n",
        "\n",
        "        if self.use_gpu:\n",
        "            Y = Y.cpu()\n",
        "        Y = Y.detach().numpy()\n",
        "        Yclasses = self.classes[Y.argmax(axis=1)].reshape((-1, 1))\n",
        "\n",
        "        return Yclasses, self._softmax(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ5G_bCEuEC2",
        "colab_type": "text"
      },
      "source": [
        "## Simple Example with Squares and Diamonds\n",
        "\n",
        "Repeating the example from lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfRjmWfsuEC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqk44FCduEC9",
        "colab_type": "code",
        "outputId": "5dbe9f4f-faa0-4087-858b-d4e39e86b83d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def make_images(nEach):\n",
        "    images = np.zeros((nEach * 2, 1, 20, 20))  # nSamples, nChannels, rows, columns\n",
        "    radii = 3 + np.random.randint(10 - 5, size=(nEach * 2, 1))\n",
        "    centers = np.zeros((nEach * 2, 2))\n",
        "    for i in range(nEach * 2):\n",
        "        r = radii[i, 0]\n",
        "        centers[i, :] = r + 1 + np.random.randint(18 - 2 * r, size=(1, 2))\n",
        "        x = int(centers[i, 0])\n",
        "        y = int(centers[i, 1])\n",
        "        if i < nEach:\n",
        "            # squares\n",
        "            images[i, 0, x - r:x + r, y + r] = 1.0\n",
        "            images[i, 0, x - r:x + r, y - r] = 1.0\n",
        "            images[i, 0, x - r, y - r:y + r] = 1.0\n",
        "            images[i, 0, x + r, y - r:y + r + 1] = 1.0\n",
        "        else:\n",
        "            # diamonds\n",
        "            images[i, 0, range(x - r, x), range(y, y + r)] = 1.0\n",
        "            images[i, 0, range(x - r, x), range(y, y - r, -1)] = 1.0\n",
        "            images[i, 0, range(x, x + r + 1), range(y + r, y - 1, -1)] = 1.0\n",
        "            images[i, 0, range(x, x + r), range(y - r, y)] = 1.0\n",
        "            # images += np.random.randn(*images.shape) * 0.5\n",
        "            T = np.ones((nEach * 2, 1))\n",
        "            T[nEach:] = 2\n",
        "    return images.astype(np.float32), T.astype(np.int)\n",
        "\n",
        "Xtrain, Ttrain = make_images(500)\n",
        "Xtest, Ttest = make_images(10)\n",
        "\n",
        "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 1, 20, 20), (1000, 1), (20, 1, 20, 20), (20, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwEDh6bzuEDC",
        "colab_type": "code",
        "outputId": "f14009d3-aaf2-44fb-c07e-d43b8e5fed7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nnet = NeuralNetwork_Convolutional(n_channels_in_image=Xtrain.shape[1],\n",
        "                                   image_size=Xtrain.shape[2],\n",
        "                                   n_units_in_conv_layers=[5],\n",
        "                                   kernels_size_and_stride=[[3,2]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[2], # 10, 10],\n",
        "                                   classes=[1, 2],\n",
        "                                   use_gpu=True)\n",
        "\n",
        "nnet.train(Xtrain, Ttrain, 100, learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 0.6857492923736572\n",
            "Epoch 50 error 0.10350113362073898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ClZEXByuEDH",
        "colab_type": "code",
        "outputId": "f60a8b99-e7b5-4430-f4af-635209b08423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(nnet.get_error_trace())\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp7RugSNuEDL",
        "colab_type": "code",
        "outputId": "3b6e3d88-6b77-4103-e01a-7ea452f12bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Yclasses, Y = nnet.use(Xtest)\n",
        "\n",
        "print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.', end='')\n",
        "print(f'  Training took {nnet.training_time:.3f} seconds.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 out of 20 test samples correctly classified.  Training took 0.363 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaKarF-KuEDP",
        "colab_type": "text"
      },
      "source": [
        "Let's see what the output of the convolutional layer produces, and the weight matrices of each unit that produce those output images.   Here are functions, `show_layer_output` and `show_layer_weights` that will do this for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22gI2osmuEDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_layer_output(nnet, X_sample, layer):\n",
        "    outputs = []\n",
        "    reg = nnet.nnet[layer * 2].register_forward_hook(\n",
        "        lambda self, i, o: outputs.append(o))\n",
        "    nnet.use(X_sample)\n",
        "    reg.remove()\n",
        "    output = outputs[0]\n",
        "\n",
        "    n_units = output.shape[1]\n",
        "    nplots = int(np.sqrt(n_units)) + 1\n",
        "    for unit in range(n_units):\n",
        "        plt.subplot(nplots, nplots, unit+1)\n",
        "        plt.imshow(output[0, unit, :, :].detach().cpu(),cmap='binary')\n",
        "        plt.axis('off')\n",
        "    return output\n",
        "\n",
        "def show_layer_weights(nnet, layer):\n",
        "    W = nnet.nnet[layer*2].weight.detach()\n",
        "    n_units = W.shape[0]\n",
        "    nplots = int(np.sqrt(n_units)) + 1\n",
        "    for unit in range(n_units):\n",
        "        plt.subplot(nplots, nplots, unit + 1)\n",
        "        plt.imshow(W[unit, 0, :, :].detach().cpu(), cmap='binary')\n",
        "        plt.axis('off')\n",
        "    return W"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXOwbLkjuEDW",
        "colab_type": "code",
        "outputId": "3a9eae7d-06a2-43c5-83b3-26a8da3177e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "X_sample = Xtest[0:1, :, :, :]\n",
        "plt.imshow(X_sample[0, 0, :, :], cmap='binary')\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 19.5, 19.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAC80lEQVR4nO3dMQrDMBAAwSj4/19WHhCDCsfRJsyU\nVnPNcmAEGnPOB9Dz3D0AcE6cECVOiBInRIkToo7FuV+5cL9x9tHmhChxQpQ4IUqcECVOiBInRIkT\nosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4\nIUqcECVOiBInRIkTosQJUavHc7nJGKfvpfIn5rz+7rTNCVHihChxQpQ4IUqcECVOiBInRIkTosQJ\nUeKEKNf3fsAnroJx3bevXNqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQo\ncUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I\nEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKE\nKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVO\niBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHi\nhChxQpQ4IUqcEHXsHoC1McbuEdjA5oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpTre5vMOXeP\nQJzNCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKU\nOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidE\niROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC\nlDghSpwQJU6IEidEiROijsX5+MoUwBubE6LECVHihChxQpQ4IUqcEPUC0qYO2FcPyc0AAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUDhaww1uEDa",
        "colab_type": "code",
        "outputId": "d8cfef73-befe-4112-c790-8ec298927596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "show_layer_output(nnet, X_sample, 0);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAACbCAYAAADydVejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYO8XAGAzLnBfrZAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHTqail7uEDe",
        "colab_type": "code",
        "outputId": "2866f4d6-f65e-487a-ddab-ff7686056638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "show_layer_weights(nnet, 0);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAACbCAYAAADydVejAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADoklEQVR4nO3dMUosaxSF0VOPG5v0IAycgUMwEAwE\nEzFwBIKIszETIwNp6EDo0MRIRDByAmKgCG1WdwLejk6h7LdW2rCroODjD5R/GMexAJL999MvADA1\noQPiCR0QT+iAeEIHxBM6IN6fdT/u7e21/+3JcrnsnqyqqsVi0b65vb09tI/+ArPZrP27Xl1ddU9W\nVdXu7m775mq1ivyuVVXn5+ft33Y+n3dPVlXV09NT++Y4jt9+Wyc6IJ7QAfGEDogndEA8oQPiCR0Q\nT+iAeEIHxBM6IJ7QAfGEDogndEA8oQPiCR0QT+iAeEIHxBM6IJ7QAfGEDogndEC8tZfj3N7etj/w\n8/OzfbOqamNjo33z4+OjffM3eHt7a988Ojpq36yq+vr6mmQ31Wq1at+8ublp36yqury8nGT3O050\nQDyhA+IJHRBP6IB4QgfEEzogntAB8YQOiCd0QDyhA+IJHRBP6IB4QgfEEzogntAB8YQOiCd0QDyh\nA+IJHRBP6IB4QgfEW3sL2N3dXfsDh2Fo36yqOjk5mWQ30f7+fvvm4eFh+2ZV1f39/SS7qaa4De/0\n9LR9s6rq+vp6kt3vONEB8YQOiCd0QDyhA+IJHRBP6IB4QgfEEzogntAB8YQOiCd0QDyhA+IJHRBP\n6IB4QgfEEzogntAB8YQOiCd0QDyhA+IN4zj+9DsATMqJDogndEA8oQPiCR0QT+iAeEIHxBM6IJ7Q\nAfGEDogndEA8oQPiCR0Q78+6HxeLRft//B8cHHRPVlXVy8tL++ZsNhvaR3+Bzc3N9u96fHzcPVlV\nVWdnZ1PMRn5X/s2JDogndEA8oQPiCR0QT+iAeEIHxBM6IJ7QAfGEDogndEA8oQPiCR0QT+iAeEIH\nxBM6IJ7QAfGEDogndEA8oQPiCR0Qb+3lOM/Pz+0PfH9/b9+sqhqG/vtOxrH9Dplf4eHhoX1zPp+3\nb1b5rvRwogPiCR0QT+iAeEIHxBM6IJ7QAfGEDogndEA8oQPiCR0QT+iAeEIHxBM6IJ7QAfGEDogn\ndEA8oQPiCR0QT+iAeEIHxBM6IN7aW8C2trbaH7izs9O+WVX1+Pg4yW6i19fX9s3lctm+WVV1cXEx\nyS7/L050QDyhA+IJHRBP6IB4QgfEEzogntAB8YQOiCd0QDyhA+IJHRBP6IB4QgfEEzogntAB8YQO\niCd0QDyhA+IJHRBP6IB4wziOP/0OAJNyogPiCR0QT+iAeEIHxBM6IJ7QAfH+ArCdaUasCX2mAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYxteEkcuEDi",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Digits\n",
        "\n",
        "Investigate the application of your code to the classification of MNIST digits, which you may download from [this site](http://deeplearning.net/tutorial/gettingstarted.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JelFnZFvuEDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, gzip\n",
        "\n",
        "# Load the dataset\n",
        "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG6qO0l9uEDn",
        "colab_type": "code",
        "outputId": "ceee96fc-4fb8-406b-9fb3-f3db27b57503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtrain = train_set[0]\n",
        "Ttrain = train_set[1]\n",
        "Xtrain.shape, Ttrain.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 784), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvlBKvpmJyjU",
        "colab_type": "code",
        "outputId": "e33ff6ec-c7dd-4b6a-db60-154dbfe7485c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtest = test_set[0]\n",
        "Ttest = test_set[1]\n",
        "Xtest.shape, Ttest.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 784), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgs8GXem_P_i",
        "colab_type": "code",
        "outputId": "2afb487b-ac0b-491e-92fa-d47ced59f115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "XV=valid_set[0]\n",
        "TV=valid_set[1]\n",
        "XV.shape,TV.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 784), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1nnnyf_MwGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_images1(nEach,Xtrain):\n",
        "    images = np.zeros((nEach * 2, 1, 28, 28))  # nSamples, nChannels, rows, columns\n",
        "    for i in range(nEach*2-1):\n",
        "      #print(i)\n",
        "      #print(Xtrain[i,:].shape)\n",
        "      a = Xtrain[i, :].reshape(28, 28)\n",
        "      #a.shape\n",
        "      images[i,0,0:28,0:28]=a[0:28,0:28]\n",
        "    return images\n",
        "    \"\"\"\"\"for i in range(nEach * 2):\n",
        "        r = radii[i, 0]\n",
        "        centers[i, :] = r + 1 + np.random.randint(18 - 2 * r, size=(1, 2))\n",
        "        x = int(centers[i, 0])\n",
        "        y = int(centers[i, 1])\n",
        "        if i < nEach:\n",
        "            # squares\n",
        "            images[i, 0, x - r:x + r, y + r] = 1.0\n",
        "            images[i, 0, x - r:x + r, y - r] = 1.0\n",
        "            images[i, 0, x - r, y - r:y + r] = 1.0\n",
        "            images[i, 0, x + r, y - r:y + r + 1] = 1.0\n",
        "        else:\n",
        "            # diamonds\n",
        "            images[i, 0, range(x - r, x), range(y, y + r)] = 1.0\n",
        "            images[i, 0, range(x - r, x), range(y, y - r, -1)] = 1.0\n",
        "            images[i, 0, range(x, x + r + 1), range(y + r, y - 1, -1)] = 1.0\n",
        "            images[i, 0, range(x, x + r), range(y - r, y)] = 1.0\n",
        "            # images += np.random.randn(*images.shape) * 0.5\n",
        "            T = np.ones((nEach * 2, 1))\n",
        "            T[nEach:] = 2\n",
        "    return images.astype(np.float32), T.astype(np.int)\n",
        "\n",
        "Xtrain, Ttrain = make_images(500)\n",
        "Xtest, Ttest = make_images(10)\n",
        "\n",
        "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzIB7DLdKP97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "P=np.unique(Ttrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lqt7tGEuEDq",
        "colab_type": "code",
        "outputId": "8787756a-e314-4b12-cd0a-bb775839a0f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = Xtrain[0, :].reshape(28, 28)\n",
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Qvb0gd9buEDu",
        "colab_type": "code",
        "outputId": "9ef2a344-51ac-497a-d9e3-5d7042e011fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.imshow(Xtrain[i, :].reshape(28,28), cmap='binary')\n",
        "    plt.title(Ttrain[i])\n",
        "    plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAD3CAYAAABfE5LaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAJVoTgBUojkBUInmBECl/wPA6cNuqHM2swAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqzEr8ScuEDy",
        "colab_type": "text"
      },
      "source": [
        "**Case 1**-Now we will first test the model with 5 units in  1 convolutional layer  with a kernel size of 3*3  and a stride of 2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6vrXVNuJNXs",
        "colab_type": "code",
        "outputId": "0639b38a-df75-46a5-e508-7cce4bdc1413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[5],\n",
        "                                   kernels_size_and_stride=[[3,2]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[2], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)\n",
        "\n",
        "Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "Xtrain1=Xtrain1.float()\n",
        "nnet.train(Xtrain1, Ttrain, 200, learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.414691925048828\n",
            "Epoch 50 error 1.5708140134811401\n",
            "Epoch 100 error 1.361205816268921\n",
            "Epoch 150 error 1.241809368133545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYCXkANuJvws",
        "colab_type": "code",
        "outputId": "cc8ec67f-84b5-4ca9-81d6-df0db9aaf4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "Xtest1=Xtest1.float()\n",
        "Yclasses, Y = nnet.use(Xtest1)\n",
        "Ttest=np.array(Ttest).reshape(10000,1)\n",
        "print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "a=np.sum(Ttest == Yclasses)/Ttest.shape[0]\n",
        "print(f'Accuracy is  {a}')\n",
        "print(f'  Training took {nnet.training_time:.3f} seconds.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4611 out of 10000 test samples correctly classified.\n",
            "Accuracy is  0.4611\n",
            "  Training took 22.364 seconds.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpFKKDwiBPHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XV=make_images1(5000,XV)\n",
        "\n",
        "XV=torch.tensor(XV).to('cuda')\n",
        "XV=XV.float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZXxRdt849nQ",
        "colab_type": "text"
      },
      "source": [
        "So accuracy is quite bad i.e. 46.11% ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5epzgRL5TtX",
        "colab_type": "text"
      },
      "source": [
        "**Case 2**\n",
        "\n",
        "Now finding the number of  layers for which the accuracy will improve . I have done a for loop for the values of the number  of the layers with only one unit in each layer. We will take the number of layers for which the accuracy will be highest or for which the increase in accuracy is not as much with respect to training time. The kernel size is taken to be 1*1 with a stride of 1.The number of epochs is taken to be 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMnYgEHeHZR2",
        "colab_type": "code",
        "outputId": "c836a08f-b48b-4903-911d-3934e4cc11da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "[[2,1] for i in range(3)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1], [2, 1], [2, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk1KP3KKsIaW",
        "colab_type": "code",
        "outputId": "03aea5cf-951a-432b-a24b-531593ee76b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "li=[]\n",
        "\n",
        "for i in range(1,11):\n",
        "  li.append(2)\n",
        "  kernel=[[2,1]for j in range(i)]\n",
        "  print(li)\n",
        "  print(kernel)\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=li,\n",
        "                                   kernels_size_and_stride=kernel,# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[3], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 100, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {i} conv layers results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n",
            "[[2, 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3496551513671875\n",
            "Epoch 50 error 1.3466198444366455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 1 conv layers results are\n",
            "6503 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.03%\n",
            "32921 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  65.842%\n",
            "6662 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  66.62%\n",
            "Training  took 29.225 seconds.\n",
            "\n",
            "\n",
            "[2, 2]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3442916870117188\n",
            "Epoch 50 error 1.3107362985610962\n",
            "\n",
            "\n",
            "So for 2 conv layers results are\n",
            "5945 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  59.45%\n",
            "29772 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  59.544%\n",
            "5946 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  59.46%\n",
            "Training  took 62.357 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3383781909942627\n",
            "Epoch 50 error 1.342264175415039\n",
            "\n",
            "\n",
            "So for 3 conv layers results are\n",
            "6263 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  62.629999999999995%\n",
            "31455 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  62.91%\n",
            "6277 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  62.77%\n",
            "Training  took 92.320 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.336409091949463\n",
            "Epoch 50 error 1.3208379745483398\n",
            "\n",
            "\n",
            "So for 4 conv layers results are\n",
            "6571 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.71000000000001%\n",
            "32986 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  65.972%\n",
            "6659 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  66.59%\n",
            "Training  took 119.506 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3366305828094482\n",
            "Epoch 50 error 1.504815936088562\n",
            "\n",
            "\n",
            "So for 5 conv layers results are\n",
            "5668 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  56.68%\n",
            "28216 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  56.432%\n",
            "5767 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  57.67%\n",
            "Training  took 148.351 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3537635803222656\n",
            "Epoch 50 error 1.5157924890518188\n",
            "\n",
            "\n",
            "So for 6 conv layers results are\n",
            "6558 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.58%\n",
            "32353 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  64.706%\n",
            "6669 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  66.69%\n",
            "Training  took 171.409 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3429455757141113\n",
            "Epoch 50 error 1.6062872409820557\n",
            "\n",
            "\n",
            "So for 7 conv layers results are\n",
            "6069 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  60.69%\n",
            "30090 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  60.18%\n",
            "6162 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  61.62%\n",
            "Training  took 195.526 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.347989559173584\n",
            "Epoch 50 error 1.4982020854949951\n",
            "\n",
            "\n",
            "So for 8 conv layers results are\n",
            "5721 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  57.21000000000001%\n",
            "27938 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  55.876000000000005%\n",
            "5602 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  56.02%\n",
            "Training  took 217.439 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3755156993865967\n",
            "Epoch 50 error 1.603405475616455\n",
            "\n",
            "\n",
            "So for 9 conv layers results are\n",
            "5294 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  52.94%\n",
            "26345 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  52.690000000000005%\n",
            "5309 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  53.09%\n",
            "Training  took 236.668 seconds.\n",
            "\n",
            "\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[[2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1], [2, 1]]\n",
            "Epoch 0 error 2.325244903564453\n",
            "Epoch 50 error 1.9847204685211182\n",
            "\n",
            "\n",
            "So for 10 conv layers results are\n",
            "6080 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  60.8%\n",
            "30078 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  60.156%\n",
            "6029 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  60.29%\n",
            "Training  took 253.475 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHKJPM4VEEtk",
        "colab_type": "text"
      },
      "source": [
        "**Case 3**\n",
        "\n",
        "\n",
        "So we see that the network works well for 1 and 2 convolutional layers as the traning is less and the accuracy is also good. Moreover the increase in number of layers may increase the accuracy but the training is also increased significantly so it is not efficient  to use mre than 2 layers. Now we will vary the units in each conv layer. So we will now test the number of units in 1 and 2 convolutional layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CUKfoQ-LSys",
        "colab_type": "text"
      },
      "source": [
        "First testing with 1 convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRmAur73MKKO",
        "colab_type": "text"
      },
      "source": [
        "So now testing with number of units in 1 convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EscVpxdEP5E",
        "colab_type": "code",
        "outputId": "121aa175-f244-454b-ea42-b209d18215cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(2,12):\n",
        "  li=[]\n",
        "\n",
        "  for j in range(1):\n",
        "    li.append(i)\n",
        "  print(li)\n",
        "  kernel=[[2,1]for j in range(1)]\n",
        "  print(kernel)\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=li,\n",
        "                                   kernels_size_and_stride=kernel,# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[3], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 100, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {i} units in 1  conv layers results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n",
            "[[2, 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.40366268157959\n",
            "Epoch 50 error 1.5331673622131348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 2 units in 1  conv layers results are\n",
            "6499 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  64.99000000000001%\n",
            "32392 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  64.78399999999999%\n",
            "6482 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  64.82%\n",
            "Training  took 29.297 seconds.\n",
            "\n",
            "\n",
            "[3]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.363572359085083\n",
            "Epoch 50 error 2.3012917041778564\n",
            "\n",
            "\n",
            "So for 3 units in 1  conv layers results are\n",
            "1135 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  11.35%\n",
            "5678 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  11.356%\n",
            "1064 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  10.639999999999999%\n",
            "Training  took 30.626 seconds.\n",
            "\n",
            "\n",
            "[4]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.3620519638061523\n",
            "Epoch 50 error 1.5199567079544067\n",
            "\n",
            "\n",
            "So for 4 units in 1  conv layers results are\n",
            "6218 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  62.18%\n",
            "31260 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  62.519999999999996%\n",
            "6261 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  62.61%\n",
            "Training  took 32.705 seconds.\n",
            "\n",
            "\n",
            "[5]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.321394443511963\n",
            "Epoch 50 error 1.5591027736663818\n",
            "\n",
            "\n",
            "So for 5 units in 1  conv layers results are\n",
            "5355 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  53.55%\n",
            "26849 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  53.698%\n",
            "5445 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  54.449999999999996%\n",
            "Training  took 34.242 seconds.\n",
            "\n",
            "\n",
            "[6]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.372035264968872\n",
            "Epoch 50 error 1.5132392644882202\n",
            "\n",
            "\n",
            "So for 6 units in 1  conv layers results are\n",
            "6585 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.85%\n",
            "33106 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  66.212%\n",
            "6628 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  66.28%\n",
            "Training  took 35.992 seconds.\n",
            "\n",
            "\n",
            "[7]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.3573379516601562\n",
            "Epoch 50 error 1.8923195600509644\n",
            "\n",
            "\n",
            "So for 7 units in 1  conv layers results are\n",
            "5771 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  57.709999999999994%\n",
            "28631 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  57.262%\n",
            "5745 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  57.45%\n",
            "Training  took 37.484 seconds.\n",
            "\n",
            "\n",
            "[8]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.3757901191711426\n",
            "Epoch 50 error 1.5590932369232178\n",
            "\n",
            "\n",
            "So for 8 units in 1  conv layers results are\n",
            "5740 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  57.4%\n",
            "28579 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  57.158%\n",
            "5808 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  58.08%\n",
            "Training  took 39.331 seconds.\n",
            "\n",
            "\n",
            "[9]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.382445812225342\n",
            "Epoch 50 error 1.4270762205123901\n",
            "\n",
            "\n",
            "So for 9 units in 1  conv layers results are\n",
            "5888 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  58.879999999999995%\n",
            "29456 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  58.912%\n",
            "5859 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  58.589999999999996%\n",
            "Training  took 40.491 seconds.\n",
            "\n",
            "\n",
            "[10]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.3509814739227295\n",
            "Epoch 50 error 2.3017284870147705\n",
            "\n",
            "\n",
            "So for 10 units in 1  conv layers results are\n",
            "1135 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  11.35%\n",
            "5678 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  11.356%\n",
            "1064 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  10.639999999999999%\n",
            "Training  took 42.604 seconds.\n",
            "\n",
            "\n",
            "[11]\n",
            "[[2, 1]]\n",
            "Epoch 0 error 2.3936569690704346\n",
            "Epoch 50 error 1.5660511255264282\n",
            "\n",
            "\n",
            "So for 11 units in 1  conv layers results are\n",
            "6125 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  61.25000000000001%\n",
            "30806 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  61.612%\n",
            "6238 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  62.38%\n",
            "Training  took 44.129 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktg2TzhLP06D",
        "colab_type": "text"
      },
      "source": [
        "**CASE 4**\n",
        "\n",
        "Now we simply take 2 units in 1 convolutional layer and test with number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaOy1n-1P7RI",
        "colab_type": "code",
        "outputId": "27ae8bef-afd6-4de8-dd6a-d0f671d6b832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "epoch=100\n",
        "for i in range(2,12):\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2],\n",
        "                                   kernels_size_and_stride=[[2,1]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[3], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, epoch, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {epoch} epochs  results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n",
        "  epoch=epoch+50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3639512062072754\n",
            "Epoch 50 error 1.290328860282898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 100 epochs  results are\n",
            "6583 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.83%\n",
            "33328 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  66.656%\n",
            "6757 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  67.57%\n",
            "Training  took 29.245 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.315722703933716\n",
            "Epoch 50 error 1.337297797203064\n",
            "Epoch 100 error 1.0114843845367432\n",
            "\n",
            "\n",
            "So for 150 epochs  results are\n",
            "7204 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  72.04%\n",
            "36507 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  73.014%\n",
            "7203 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  72.03%\n",
            "Training  took 45.698 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3925657272338867\n",
            "Epoch 50 error 2.3017258644104004\n",
            "Epoch 100 error 2.3010098934173584\n",
            "Epoch 150 error 2.301027774810791\n",
            "\n",
            "\n",
            "So for 200 epochs  results are\n",
            "1135 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  11.35%\n",
            "5678 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  11.356%\n",
            "1064 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  10.639999999999999%\n",
            "Training  took 61.813 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3687281608581543\n",
            "Epoch 50 error 2.3011226654052734\n",
            "Epoch 100 error 2.301027297973633\n",
            "Epoch 150 error 2.300995349884033\n",
            "Epoch 200 error 1.7550426721572876\n",
            "\n",
            "\n",
            "So for 250 epochs  results are\n",
            "6709 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  67.09%\n",
            "33584 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  67.168%\n",
            "6790 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  67.9%\n",
            "Training  took 78.348 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3784494400024414\n",
            "Epoch 50 error 1.4240299463272095\n",
            "Epoch 100 error 1.070957064628601\n",
            "Epoch 150 error 0.9362789988517761\n",
            "Epoch 200 error 0.8670647144317627\n",
            "Epoch 250 error 0.8025599122047424\n",
            "\n",
            "\n",
            "So for 300 epochs  results are\n",
            "7225 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  72.25%\n",
            "36692 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  73.384%\n",
            "7326 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  73.26%\n",
            "Training  took 95.558 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3385684490203857\n",
            "Epoch 50 error 1.4059089422225952\n",
            "Epoch 100 error 1.1541305780410767\n",
            "Epoch 150 error 1.04330313205719\n",
            "Epoch 200 error 0.9309444427490234\n",
            "Epoch 250 error 0.79107266664505\n",
            "Epoch 300 error 0.7234886884689331\n",
            "\n",
            "\n",
            "So for 350 epochs  results are\n",
            "7661 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  76.61%\n",
            "39222 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  78.444%\n",
            "7740 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  77.4%\n",
            "Training  took 112.137 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.317625045776367\n",
            "Epoch 50 error 1.368354320526123\n",
            "Epoch 100 error 1.0989378690719604\n",
            "Epoch 150 error 0.9332922697067261\n",
            "Epoch 200 error 0.819385290145874\n",
            "Epoch 250 error 0.7281761169433594\n",
            "Epoch 300 error 0.6611006855964661\n",
            "Epoch 350 error 0.6267734169960022\n",
            "\n",
            "\n",
            "So for 400 epochs  results are\n",
            "7948 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  79.47999999999999%\n",
            "40608 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  81.216%\n",
            "7936 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  79.36%\n",
            "Training  took 128.746 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3686327934265137\n",
            "Epoch 50 error 1.6846561431884766\n",
            "Epoch 100 error 1.4773342609405518\n",
            "Epoch 150 error 1.3904730081558228\n",
            "Epoch 200 error 1.2357685565948486\n",
            "Epoch 250 error 0.9625461101531982\n",
            "Epoch 300 error 0.8671468496322632\n",
            "Epoch 350 error 0.8228622674942017\n",
            "Epoch 400 error 0.782122790813446\n",
            "\n",
            "\n",
            "So for 450 epochs  results are\n",
            "7248 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  72.48%\n",
            "36974 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  73.94800000000001%\n",
            "7401 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  74.00999999999999%\n",
            "Training  took 145.315 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.354529619216919\n",
            "Epoch 50 error 1.3469421863555908\n",
            "Epoch 100 error 1.0470433235168457\n",
            "Epoch 150 error 0.9195306301116943\n",
            "Epoch 200 error 0.8306564688682556\n",
            "Epoch 250 error 0.7635018825531006\n",
            "Epoch 300 error 0.7222825884819031\n",
            "Epoch 350 error 0.6798458695411682\n",
            "Epoch 400 error 0.6382759213447571\n",
            "Epoch 450 error 0.6002869009971619\n",
            "\n",
            "\n",
            "So for 500 epochs  results are\n",
            "7940 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  79.4%\n",
            "40452 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  80.904%\n",
            "8014 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  80.14%\n",
            "Training  took 161.717 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.356757879257202\n",
            "Epoch 50 error 1.4319676160812378\n",
            "Epoch 100 error 1.1406867504119873\n",
            "Epoch 150 error 0.9925715327262878\n",
            "Epoch 200 error 0.8600598573684692\n",
            "Epoch 250 error 0.7829201817512512\n",
            "Epoch 300 error 0.7357500195503235\n",
            "Epoch 350 error 0.7320231199264526\n",
            "Epoch 400 error 0.6905463337898254\n",
            "Epoch 450 error 0.699603259563446\n",
            "Epoch 500 error 0.6442445516586304\n",
            "\n",
            "\n",
            "So for 550 epochs  results are\n",
            "8072 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  80.72%\n",
            "41254 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  82.50800000000001%\n",
            "8161 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  81.61%\n",
            "Training  took 178.555 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf-MdogIW-TS",
        "colab_type": "text"
      },
      "source": [
        "**Case 5**\n",
        "\n",
        "\n",
        "So a network with 1 Convolutional layer with 2 units in the conv layer  and 350 epochs gives us a good result. SO now we will see what happens if we change the number of hidden units in the fully connected layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2bswtg2XS1f",
        "colab_type": "code",
        "outputId": "67e9129c-5908-42e6-df06-4a8c0ad65b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "for i in range(3,7):\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2],\n",
        "                                   kernels_size_and_stride=[[2,1]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[i], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 350, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {i} units in fully conn layer   results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3538200855255127\n",
            "Epoch 50 error 1.3259475231170654\n",
            "Epoch 100 error 1.0811452865600586\n",
            "Epoch 150 error 0.9880410432815552\n",
            "Epoch 200 error 0.9255720973014832\n",
            "Epoch 250 error 0.8439635038375854\n",
            "Epoch 300 error 0.8152079582214355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 3 units in fully conn layer   results are\n",
            "7527 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  75.27000000000001%\n",
            "38370 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  76.74%\n",
            "7681 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  76.81%\n",
            "Training  took 112.260 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3303017616271973\n",
            "Epoch 50 error 1.0728224515914917\n",
            "Epoch 100 error 0.7751679420471191\n",
            "Epoch 150 error 0.6303536891937256\n",
            "Epoch 200 error 0.5487124919891357\n",
            "Epoch 250 error 0.4993182122707367\n",
            "Epoch 300 error 0.463351845741272\n",
            "\n",
            "\n",
            "So for 4 units in fully conn layer   results are\n",
            "8601 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  86.00999999999999%\n",
            "44002 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  88.004%\n",
            "8658 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  86.58%\n",
            "Training  took 112.246 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.359452962875366\n",
            "Epoch 50 error 0.9292728900909424\n",
            "Epoch 100 error 0.6919137239456177\n",
            "Epoch 150 error 0.544629693031311\n",
            "Epoch 200 error 0.4443359375\n",
            "Epoch 250 error 0.3800577223300934\n",
            "Epoch 300 error 0.33630964159965515\n",
            "\n",
            "\n",
            "So for 5 units in fully conn layer   results are\n",
            "8951 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  89.51%\n",
            "45896 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  91.792%\n",
            "9041 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  90.41%\n",
            "Training  took 112.199 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.327336072921753\n",
            "Epoch 50 error 1.2736804485321045\n",
            "Epoch 100 error 0.7061929702758789\n",
            "Epoch 150 error 0.5281133055686951\n",
            "Epoch 200 error 0.4214898943901062\n",
            "Epoch 250 error 0.342306524515152\n",
            "Epoch 300 error 0.2874816358089447\n",
            "\n",
            "\n",
            "So for 6 units in fully conn layer   results are\n",
            "9123 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  91.23%\n",
            "46747 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  93.494%\n",
            "9165 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  91.64999999999999%\n",
            "Training  took 112.324 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Qfo35QZoY0",
        "colab_type": "text"
      },
      "source": [
        "**Case 6**\n",
        "\n",
        "So now just testing the learning rate of the network with 1 conv layer , 6 units in 1 layer,350 epochs and  6 units in fully connected layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w8oSyDqZnro",
        "colab_type": "code",
        "outputId": "a4e11397-0b54-4b65-873a-4c3da79a3719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "lr=0.01\n",
        "for i in range(2):\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2],\n",
        "                                   kernels_size_and_stride=[[2,1]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[6], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 350, learning_rate=lr)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for learnng rate {lr}    results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n",
        "  lr=lr*10\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3285634517669678\n",
            "Epoch 50 error 0.8853269815444946\n",
            "Epoch 100 error 0.5329335927963257\n",
            "Epoch 150 error 0.37594345211982727\n",
            "Epoch 200 error 0.29101866483688354\n",
            "Epoch 250 error 0.24838802218437195\n",
            "Epoch 300 error 0.21973566710948944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for learnng rate 0.01    results are\n",
            "9206 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  92.06%\n",
            "46773 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  93.54599999999999%\n",
            "9242 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  92.42%\n",
            "Training  took 112.194 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3055317401885986\n",
            "Epoch 50 error 0.6238270998001099\n",
            "Epoch 100 error 0.5024508833885193\n",
            "Epoch 150 error 0.4186032712459564\n",
            "Epoch 200 error 0.3848544657230377\n",
            "Epoch 250 error 0.364090234041214\n",
            "Epoch 300 error 0.3415069878101349\n",
            "\n",
            "\n",
            "So for learnng rate 0.1    results are\n",
            "8874 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  88.74%\n",
            "45515 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  91.03%\n",
            "8915 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  89.14999999999999%\n",
            "Training  took 111.375 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLxuEsBSfK6U",
        "colab_type": "text"
      },
      "source": [
        "**Case 7**\n",
        "\n",
        "So we achieved a 90% accuracy of classification with 1 convolutional layer, 2 units in tthe conv layer ,6 units in  the fully connected layer with a filter of 2*2 with 1 stride and learning rate of 0.01 with a training time of 112.94 sec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "555_nfWSPDdd",
        "colab_type": "text"
      },
      "source": [
        "Now doing the same with 2 convolutional layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GLRvi_mfjf9",
        "colab_type": "text"
      },
      "source": [
        "Now testing with number of units in each conv layer in a 2 conv network keeping number of units in the fully connected network as 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y4qsDyGPHtW",
        "colab_type": "code",
        "outputId": "0160b27b-ee66-45c9-e41b-264a28f35dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(2,12):\n",
        "  li=[]\n",
        "\n",
        "  for j in range(2):\n",
        "    li.append(i)\n",
        "  print(li)\n",
        "  kernel=[[2,1]for j in range(2)]\n",
        "  print(kernel)\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=li,\n",
        "                                   kernels_size_and_stride=kernel,# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[3], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 100, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {i} units in 2  conv layers results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 2]\n",
            "[[2, 1], [2, 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.360473155975342\n",
            "Epoch 50 error 1.4015876054763794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 2 units in 2  conv layers results are\n",
            "6598 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.98%\n",
            "32757 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  65.514%\n",
            "6617 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  66.17%\n",
            "Training  took 62.307 seconds.\n",
            "\n",
            "\n",
            "[3, 3]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.38254976272583\n",
            "Epoch 50 error 1.446295976638794\n",
            "\n",
            "\n",
            "So for 3 units in 2  conv layers results are\n",
            "5930 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  59.3%\n",
            "29765 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  59.53000000000001%\n",
            "5991 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  59.91%\n",
            "Training  took 67.247 seconds.\n",
            "\n",
            "\n",
            "[4, 4]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.345228672027588\n",
            "Epoch 50 error 1.3831065893173218\n",
            "\n",
            "\n",
            "So for 4 units in 2  conv layers results are\n",
            "6516 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  65.16%\n",
            "32561 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  65.122%\n",
            "6508 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  65.08%\n",
            "Training  took 70.203 seconds.\n",
            "\n",
            "\n",
            "[5, 5]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.351313591003418\n",
            "Epoch 50 error 1.3427245616912842\n",
            "\n",
            "\n",
            "So for 5 units in 2  conv layers results are\n",
            "6752 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  67.52%\n",
            "33944 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  67.888%\n",
            "6762 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  67.62%\n",
            "Training  took 74.357 seconds.\n",
            "\n",
            "\n",
            "[6, 6]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3776936531066895\n",
            "Epoch 50 error 1.4655307531356812\n",
            "\n",
            "\n",
            "So for 6 units in 2  conv layers results are\n",
            "6190 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  61.9%\n",
            "30904 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  61.80799999999999%\n",
            "6226 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  62.260000000000005%\n",
            "Training  took 77.869 seconds.\n",
            "\n",
            "\n",
            "[7, 7]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3755569458007812\n",
            "Epoch 50 error 1.3983137607574463\n",
            "\n",
            "\n",
            "So for 7 units in 2  conv layers results are\n",
            "5500 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  55.00000000000001%\n",
            "27270 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  54.54%\n",
            "5468 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  54.67999999999999%\n",
            "Training  took 83.842 seconds.\n",
            "\n",
            "\n",
            "[8, 8]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3536300659179688\n",
            "Epoch 50 error 1.4808123111724854\n",
            "\n",
            "\n",
            "So for 8 units in 2  conv layers results are\n",
            "5169 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  51.690000000000005%\n",
            "26088 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  52.176%\n",
            "5219 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  52.190000000000005%\n",
            "Training  took 87.955 seconds.\n",
            "\n",
            "\n",
            "[9, 9]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.401818037033081\n",
            "Epoch 50 error 2.3017117977142334\n",
            "\n",
            "\n",
            "So for 9 units in 2  conv layers results are\n",
            "1135 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  11.35%\n",
            "5678 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  11.356%\n",
            "1064 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  10.639999999999999%\n",
            "Training  took 91.550 seconds.\n",
            "\n",
            "\n",
            "[10, 10]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.36820912361145\n",
            "Epoch 50 error 1.5134384632110596\n",
            "\n",
            "\n",
            "So for 10 units in 2  conv layers results are\n",
            "6634 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  66.34%\n",
            "33237 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  66.474%\n",
            "6729 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  67.29%\n",
            "Training  took 96.062 seconds.\n",
            "\n",
            "\n",
            "[11, 11]\n",
            "[[2, 1], [2, 1]]\n",
            "Epoch 0 error 2.3710172176361084\n",
            "Epoch 50 error 2.0609569549560547\n",
            "\n",
            "\n",
            "So for 11 units in 2  conv layers results are\n",
            "2096 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  20.96%\n",
            "10433 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  20.866%\n",
            "1980 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  19.8%\n",
            "Training  took 100.726 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmNOFl23kett",
        "colab_type": "text"
      },
      "source": [
        "**Case 8**\n",
        "\n",
        "So the efficient result can be seen for 2 units in the conv layer as it had good accuracy and sufficient training time among others. Now testing with number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6PgpEavkjTP",
        "colab_type": "code",
        "outputId": "7e165b9c-ade8-45f6-d849-7e23c013dff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "epoch=100\n",
        "for i in range(2,12):\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2,2],\n",
        "                                   kernels_size_and_stride=[[2,1],[2,1]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[3], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, epoch, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {epoch} epochs  results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n",
        "  epoch=epoch+50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.336493730545044\n",
            "Epoch 50 error 1.381710410118103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 100 epochs  results are\n",
            "6713 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  67.13%\n",
            "33364 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  66.728%\n",
            "6821 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  68.21000000000001%\n",
            "Training  took 62.289 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3721635341644287\n",
            "Epoch 50 error 1.340882658958435\n",
            "Epoch 100 error 0.9981932044029236\n",
            "\n",
            "\n",
            "So for 150 epochs  results are\n",
            "7463 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  74.63%\n",
            "37515 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  75.03%\n",
            "7493 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  74.92999999999999%\n",
            "Training  took 96.537 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3812716007232666\n",
            "Epoch 50 error 1.7299443483352661\n",
            "Epoch 100 error 1.3502389192581177\n",
            "Epoch 150 error 1.115965723991394\n",
            "\n",
            "\n",
            "So for 200 epochs  results are\n",
            "6144 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  61.44%\n",
            "30814 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  61.62800000000001%\n",
            "6140 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  61.4%\n",
            "Training  took 130.830 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3437633514404297\n",
            "Epoch 50 error 1.3974223136901855\n",
            "Epoch 100 error 1.1295268535614014\n",
            "Epoch 150 error 0.968198299407959\n",
            "Epoch 200 error 0.8683720827102661\n",
            "\n",
            "\n",
            "So for 250 epochs  results are\n",
            "7154 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  71.54%\n",
            "36290 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  72.58%\n",
            "7228 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  72.28%\n",
            "Training  took 165.023 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3520822525024414\n",
            "Epoch 50 error 1.318890929222107\n",
            "Epoch 100 error 1.0412992238998413\n",
            "Epoch 150 error 0.9193332195281982\n",
            "Epoch 200 error 0.7634876370429993\n",
            "Epoch 250 error 0.6883634328842163\n",
            "\n",
            "\n",
            "So for 300 epochs  results are\n",
            "7429 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  74.29%\n",
            "37657 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  75.31400000000001%\n",
            "7448 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  74.48%\n",
            "Training  took 199.446 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.341965913772583\n",
            "Epoch 50 error 1.372312307357788\n",
            "Epoch 100 error 1.154746651649475\n",
            "Epoch 150 error 0.991383969783783\n",
            "Epoch 200 error 0.8382658362388611\n",
            "Epoch 250 error 0.7387327551841736\n",
            "Epoch 300 error 0.6984638571739197\n",
            "\n",
            "\n",
            "So for 350 epochs  results are\n",
            "7830 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  78.3%\n",
            "39565 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  79.13%\n",
            "7903 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  79.03%\n",
            "Training  took 233.658 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.350461483001709\n",
            "Epoch 50 error 2.173806667327881\n",
            "Epoch 100 error 1.3872393369674683\n",
            "Epoch 150 error 1.0705276727676392\n",
            "Epoch 200 error 0.9413224458694458\n",
            "Epoch 250 error 0.8162907958030701\n",
            "Epoch 300 error 0.7578274011611938\n",
            "Epoch 350 error 0.6846466660499573\n",
            "\n",
            "\n",
            "So for 400 epochs  results are\n",
            "7735 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  77.35%\n",
            "39007 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  78.014%\n",
            "7780 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  77.8%\n",
            "Training  took 267.890 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.358464479446411\n",
            "Epoch 50 error 1.5683501958847046\n",
            "Epoch 100 error 1.1416354179382324\n",
            "Epoch 150 error 0.9653353095054626\n",
            "Epoch 200 error 0.863123893737793\n",
            "Epoch 250 error 0.7851245999336243\n",
            "Epoch 300 error 0.7278006076812744\n",
            "Epoch 350 error 0.6797915101051331\n",
            "Epoch 400 error 0.6396293640136719\n",
            "\n",
            "\n",
            "So for 450 epochs  results are\n",
            "8044 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  80.44%\n",
            "40556 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  81.112%\n",
            "8057 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  80.57%\n",
            "Training  took 302.010 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3747246265411377\n",
            "Epoch 50 error 1.4085170030593872\n",
            "Epoch 100 error 1.0938808917999268\n",
            "Epoch 150 error 0.9664468169212341\n",
            "Epoch 200 error 0.8316746354103088\n",
            "Epoch 250 error 0.7161952257156372\n",
            "Epoch 300 error 0.6521714329719543\n",
            "Epoch 350 error 0.6097350120544434\n",
            "Epoch 400 error 0.5937713384628296\n",
            "Epoch 450 error 0.5705493688583374\n",
            "\n",
            "\n",
            "So for 500 epochs  results are\n",
            "8292 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  82.92%\n",
            "41949 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  83.898%\n",
            "8227 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  82.27%\n",
            "Training  took 336.422 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3495798110961914\n",
            "Epoch 50 error 1.4340393543243408\n",
            "Epoch 100 error 1.0774997472763062\n",
            "Epoch 150 error 0.9515136480331421\n",
            "Epoch 200 error 0.8606533408164978\n",
            "Epoch 250 error 0.7936198711395264\n",
            "Epoch 300 error 0.7370128035545349\n",
            "Epoch 350 error 0.6942323446273804\n",
            "Epoch 400 error 0.6727807521820068\n",
            "Epoch 450 error 0.6570490598678589\n",
            "Epoch 500 error 0.6422588229179382\n",
            "\n",
            "\n",
            "So for 550 epochs  results are\n",
            "7750 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  77.5%\n",
            "39454 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  78.908%\n",
            "7818 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  78.18%\n",
            "Training  took 370.737 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs5pa0sdrEZH",
        "colab_type": "text"
      },
      "source": [
        "**Case 9**\n",
        "\n",
        "so we take 350 epochs  2 conv layers with 2 units in each layer. Now we test the number of units in the fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAT5E0CirURC",
        "colab_type": "code",
        "outputId": "b9f61adf-90b5-4078-ba8d-ca7e7a11ed80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "for i in range(3,7):\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2,2],\n",
        "                                   kernels_size_and_stride=[[2,1],[2,1]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[i], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 350, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for {i} units in fully conn layer   results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.334127187728882\n",
            "Epoch 50 error 1.5128860473632812\n",
            "Epoch 100 error 1.1356557607650757\n",
            "Epoch 150 error 0.9761335253715515\n",
            "Epoch 200 error 0.9022238850593567\n",
            "Epoch 250 error 0.8380777835845947\n",
            "Epoch 300 error 0.7280965447425842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for 3 units in fully conn layer   results are\n",
            "7927 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  79.27%\n",
            "40024 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  80.048%\n",
            "7966 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  79.66%\n",
            "Training  took 233.568 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.357254981994629\n",
            "Epoch 50 error 1.207971453666687\n",
            "Epoch 100 error 0.8971073627471924\n",
            "Epoch 150 error 0.6816371083259583\n",
            "Epoch 200 error 0.5682219862937927\n",
            "Epoch 250 error 0.46468308568000793\n",
            "Epoch 300 error 0.4278651177883148\n",
            "\n",
            "\n",
            "So for 4 units in fully conn layer   results are\n",
            "8818 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  88.18%\n",
            "44528 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  89.056%\n",
            "8837 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  88.37%\n",
            "Training  took 233.625 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.341076135635376\n",
            "Epoch 50 error 1.151045560836792\n",
            "Epoch 100 error 0.6994232535362244\n",
            "Epoch 150 error 0.5609184503555298\n",
            "Epoch 200 error 0.4840810298919678\n",
            "Epoch 250 error 0.40317636728286743\n",
            "Epoch 300 error 0.32341626286506653\n",
            "\n",
            "\n",
            "So for 5 units in fully conn layer   results are\n",
            "9197 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  91.97%\n",
            "46454 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  92.908%\n",
            "9189 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  91.89%\n",
            "Training  took 233.523 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.3600218296051025\n",
            "Epoch 50 error 1.1173592805862427\n",
            "Epoch 100 error 0.6357858180999756\n",
            "Epoch 150 error 0.47456082701683044\n",
            "Epoch 200 error 0.37516582012176514\n",
            "Epoch 250 error 0.30220574140548706\n",
            "Epoch 300 error 0.24925671517848969\n",
            "\n",
            "\n",
            "So for 6 units in fully conn layer   results are\n",
            "9248 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  92.47999999999999%\n",
            "47189 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  94.378%\n",
            "9326 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  93.26%\n",
            "Training  took 233.661 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U22aU2Bh13z6",
        "colab_type": "text"
      },
      "source": [
        "**Case 10**\n",
        "\n",
        "\n",
        "Now we will test with the learning rates. So the network will be of 2 conv layers, with 2 units in each layer , with 6 units in the hidden connected layer and 350 epochs with a filter of 2*2 with a stride of 1 ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6qUW0Ko2Rsc",
        "colab_type": "code",
        "outputId": "b80a37c0-6d28-4c51-968e-cf5d84f79555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "lr=0.01\n",
        "for i in range(2):\n",
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2,2],\n",
        "                                   kernels_size_and_stride=[[2,1],[2,1]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[6], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 350, learning_rate=lr)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'So for learnng rate {lr}    results are')\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()\n",
        "  lr=lr*10\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3244075775146484\n",
            "Epoch 50 error 0.9444521069526672\n",
            "Epoch 100 error 0.6012100577354431\n",
            "Epoch 150 error 0.4696700870990753\n",
            "Epoch 200 error 0.37455612421035767\n",
            "Epoch 250 error 0.29774120450019836\n",
            "Epoch 300 error 0.24946387112140656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "So for learnng rate 0.01    results are\n",
            "9339 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  93.39%\n",
            "47162 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  94.324%\n",
            "9319 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  93.19%\n",
            "Training  took 233.737 seconds.\n",
            "\n",
            "\n",
            "Epoch 0 error 2.331159830093384\n",
            "Epoch 50 error 2.3015787601470947\n",
            "Epoch 100 error 2.3010048866271973\n",
            "Epoch 150 error 2.301020860671997\n",
            "Epoch 200 error 2.30102801322937\n",
            "Epoch 250 error 2.3010034561157227\n",
            "Epoch 300 error 2.3010034561157227\n",
            "\n",
            "\n",
            "So for learnng rate 0.1    results are\n",
            "1135 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  11.35%\n",
            "5678 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  11.356%\n",
            "1064 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  10.639999999999999%\n",
            "Training  took 231.269 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS2NeBGr4WuT",
        "colab_type": "text"
      },
      "source": [
        "So we see that a network with 1 convolutional layer with 2 units in each layer with 6 units in the fully connected layer and a learning rate of 0.01 gives an accuracy of 92.06% on test dataset ,93.55% on train dataset and 92.42% on validation dataset.\n",
        "Accuracy of test data   is  92.06% with a training time of 112.169 sec.\n",
        "\n",
        "On the other hand we see that a network with 2 convolutional layers , with 2 units in each layer and with 6 units in the fully connected layer and a learning rate  of 0.01 gives an acuracy of 93.39% on test dataset ,94.32% on train dataset and  93.19% on validation dataset with a training time of 233.737 sec.\n",
        "\n",
        "\n",
        "Both of them use the same filter i.e. 2*2 with a strde of 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1owVXjnj63_W",
        "colab_type": "text"
      },
      "source": [
        "So although the accuracy increased a bit in the network havng 2 conv layers , that increase does not justify the increase in training time. \n",
        "\n",
        "\n",
        "So the network that works well is a network with 1 convolutional layer with 2 units in each layer with 6 units in the fully connected layer and a learning rate of 0.01 .\n",
        "\n",
        "but again if we change the kernel and stride we can get a different value . The following has been shown in the extra credit part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVgj9ijYCFdb",
        "colab_type": "text"
      },
      "source": [
        "So **Case 11** has been shown in the extra credit part 1 with a different kernel size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKOeDVvuEDz",
        "colab_type": "text"
      },
      "source": [
        "# Grading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyf-PCeuED0",
        "colab_type": "code",
        "outputId": "dc4f9f57-f948-418b-bbec-024722a0590e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#%run -i A4grader.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======================= Code Execution =======================\n",
            "\n",
            "Extracting python code from notebook named 'Roy-A4.ipynb' and storing in notebookcode.py\n",
            "Removing all statements that are not function or class defs or import statements.\n",
            "\n",
            "Testing if your NeuralNetwork_Convolutional can learn to classify a small \n",
            "subset of hand_drawn 0, 1, and 2 digits.\n",
            "\n",
            "import numpy as np\n",
            "import pickle, gzip\n",
            "\n",
            "# Load the dataset\n",
            "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
            "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
            "Xtest = test_set[0]\n",
            "traini = [3,  10,  13,  25,  28,  55,  69,  71, 101, 126, 2,   5,  14,  29,  31,  37,  39,  40,  46,  57, 1,  35,  38,  43,  47,  72,  77,  82, 106, 119]\n",
            "testi = [136, 148, 157, 183, 188, 192, 194, 215, 246, 269,  74,  89,  94, 96, 107, 135, 137, 143, 145, 154, 147, 149, 172, 174, 186, 199, 208, 221, 222, 225]\n",
            "Xtrain = test_set[0][traini, :].reshape(-1, 1, 28, 28)\n",
            "Ttrain = test_set[1][traini].reshape(-1, 1)\n",
            "Xtest = test_set[0][testi, :].reshape(-1, 1, 28, 28)\n",
            "Ttest = test_set[1][testi].reshape(-1, 1)\n",
            "\n",
            "torch.random.manual_seed(42)\n",
            "\n",
            "nnet = NeuralNetwork_Convolutional(n_channels_in_image=Xtrain.shape[1],\n",
            "                                   image_size=Xtrain.shape[2],\n",
            "                                   n_units_in_conv_layers=[5, 5],\n",
            "                                   kernels_size_and_stride=[[5, 3], [4, 2]],\n",
            "                                   n_units_in_fc_hidden_layers=[10],\n",
            "                                   classes=[0, 1, 2],\n",
            "                                   use_gpu=False)\n",
            "\n",
            "nnet.train(Xtrain, Ttrain, 20, learning_rate=0.01)\n",
            "Yclasses, Y = nnet.use(Xtest)\n",
            "n_correct = (Yclasses == Ttest).sum()\n",
            "print(f'{n_correct} out of {Ttest.shape[0]} samples, or {n_correct/Ttest.shape[0]*100:.2f} percent.')\n",
            "\n",
            "Epoch 0 error 1.1164677143096924\n",
            "Epoch 10 error 0.3168463706970215\n",
            "27 out of 30 samples, or 90.00 percent.\n",
            "\n",
            "--- 80/80 points. Returned correct value of 27.\n",
            "\n",
            "Testing \n",
            "errors = nnet.get_error_trace()\n",
            "\n",
            "\n",
            "--- 10/10 points. Returned correct number of errors in error_trace.\n",
            "\n",
            "======================================================================\n",
            "content Execution Grade is 90 / 90\n",
            "======================================================================\n",
            "\n",
            " __ / 10 You show results for at least 10 different runs having various\n",
            "values for number of layers, units, and epochs for modeling MNIST data.\n",
            "You must show percent correct for train, validation and test sets for each run.\n",
            "Discuss your results. \n",
            "\n",
            "======================================================================\n",
            "content FINAL GRADE is  _  / 100\n",
            "======================================================================\n",
            "\n",
            "Extra Credit:\n",
            "1. For one of your runs, display the output images of your convolutional layers \n",
            "   and the weights for those layers.  Discuss what you see.  Describe why the\n",
            "   displayed weight patterns result in the output images of the first convolutional\n",
            "   layer.\n",
            "2. Make at least one of your runs on a workstation with a GPU and run with use_gpu=True.\n",
            "   Also run it with use_gpu=False.  Discuss the differences in training times.\n",
            "\n",
            "content EXTRA CREDIT is 0 / 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wQoXtNywy_w",
        "colab_type": "text"
      },
      "source": [
        "Extra Credit 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CukKaAb8U9k",
        "colab_type": "text"
      },
      "source": [
        "We make a network with 2 convolutional layers, the layer containing 2 units and the second layer containing 3 units  , with 1 fully connected layers with 6 units each and with  a filter of size 3*3 with a stride of 2 and with 500 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H5S2yqd7nBj",
        "colab_type": "code",
        "outputId": "76d2323a-37df-46f2-c44b-da9d40001fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "  nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[2,3],\n",
        "                                   kernels_size_and_stride=[[3,2],[3,2]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[6], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)  \n",
        "\n",
        "\n",
        "  Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "  Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "  Xtrain1=Xtrain1.float()\n",
        "  nnet.train(Xtrain1, Ttrain, 500, learning_rate=0.01)\n",
        "  Xtest1=make_images1(5000,Xtest)\n",
        "\n",
        "  Xtest1=torch.tensor(Xtest1).to('cuda')\n",
        "  Xtest1=Xtest1.float()\n",
        "  Yclasses, Y = nnet.use(Xtest1)\n",
        "  Ttest=np.array(Ttest).reshape(10000,1)\n",
        "  print()\n",
        "  print()\n",
        "  print(f'{np.sum(Ttest == Yclasses)} out of {Ttest.shape[0]} test samples correctly classified.')\n",
        "  a=np.sum(Ttest == Yclasses)/Ttest.shape[0]*100\n",
        "  print(f'Accuracy of test data   is  {a}%')\n",
        "  \n",
        "  \n",
        "  Yclasses1, Y1 = nnet.use(Xtrain1)\n",
        "  Ttrain=np.array(Ttrain).reshape(50000,1)\n",
        "  print(f'{np.sum(Ttrain == Yclasses1)} out of {Ttrain.shape[0]} test samples correctly classified.')\n",
        "  a1=np.sum(Ttrain == Yclasses1)/Ttrain.shape[0]*100\n",
        "  print(f'Accuracy of train data   is  {a1}%')\n",
        "  \n",
        "  Yclasses2, Y2 = nnet.use(XV)\n",
        "  TV=np.array(TV).reshape(10000,1)\n",
        "  print(f'{np.sum(TV == Yclasses2)} out of {TV.shape[0]} test samples correctly classified.')\n",
        "  a2=np.sum(TV == Yclasses2)/TV.shape[0]*100\n",
        "  print(f'Accuracy of validation data   is  {a2}%')\n",
        "  print(f'Training  took {nnet.training_time:.3f} seconds.')\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.365222692489624\n",
            "Epoch 50 error 0.9488264322280884\n",
            "Epoch 100 error 0.559618353843689\n",
            "Epoch 150 error 0.4464861750602722\n",
            "Epoch 200 error 0.3748335540294647\n",
            "Epoch 250 error 0.3235929608345032\n",
            "Epoch 300 error 0.29146867990493774\n",
            "Epoch 350 error 0.2690604031085968\n",
            "Epoch 400 error 0.25386032462120056\n",
            "Epoch 450 error 0.23886612057685852\n",
            "\n",
            "\n",
            "9364 out of 10000 test samples correctly classified.\n",
            "Accuracy of test data   is  93.64%\n",
            "46764 out of 50000 test samples correctly classified.\n",
            "Accuracy of train data   is  93.528%\n",
            "9422 out of 10000 test samples correctly classified.\n",
            "Accuracy of validation data   is  94.22%\n",
            "Training  took 74.999 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o5a_IQa98ow",
        "colab_type": "text"
      },
      "source": [
        "So we see an accuracy of 93.64% on test datatset and 93.528% on train dataset adn 94.22% on validation datatset with a training time of 74.99 sec which is by far  beats the previous networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr7dGAngw0yC",
        "colab_type": "code",
        "outputId": "91bbe1d5-691e-40d3-e51d-e96a824b92fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "X_sample = Xtest1[0:1, :, :, :]\n",
        "plt.imshow(X_sample[0, 0, :, :].detach().cpu(), cmap='binary')\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBdh5npwQSpwQSpwQSpwQSpwQSpwQ6n/eRcG/yvhmRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4-lnYOi-Uzf",
        "colab_type": "text"
      },
      "source": [
        "Now showing the output of first convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJdfOZ54zh10",
        "colab_type": "code",
        "outputId": "577437aa-b531-4443-df8e-05af55351f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "print('output  of first convolutional layer')\n",
        "show_layer_output(nnet, X_sample, 0);\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output  of first convolutional layer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAABxCAYAAACX+mUfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhJIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73EfSe0X-aAc",
        "colab_type": "text"
      },
      "source": [
        "So as the first convolutional layer has 2 units  it  shows 2 ouputs. \n",
        "\n",
        "\n",
        "The first unit applies the kernel on the input image.\n",
        "\n",
        "\n",
        "The second unit applies the kernel on the input image "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm9JmVEN-rtw",
        "colab_type": "text"
      },
      "source": [
        "Now showing the output of the second convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoG-aSOjzw68",
        "colab_type": "code",
        "outputId": "224deca8-aa12-4de5-f049-7bddcdcc71f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "print('output  of second convolutional layer')\n",
        "show_layer_output(nnet, X_sample, 1);\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output  of second convolutional layer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAADnCAYAAABcxZBBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAF6klEQVR4nO3cP0vVbRzH8euUtKgUtiQ4pNTQEkbP\noIZoCYMeQKs9BCcfSEFbhNAWDUlRU039HYuGpDwWkuIgeeq03fdQ/hq+3vi58fVaD9+LL/LjzQXC\n1RsOhw0gzaH9XgDgT8QJiCROQCRxAiKJExBppOvHubm50r/yBoNBZXxPjI6Ols84fPhwaX5nZ6e8\nw9LSUq98CP9YWFgofdsTExPlHaampkrzY2Nj5R0+ffq0r/Ottba4uPjHb9vNCYgkTkAkcQIiiRMQ\nSZyASOIERBInIJI4AZHECYgkTkAkcQIiiRMQSZyASOIERBInIFLne06HDtXadf/+/dJ8a63dvHmz\nNH/27NnyDuvr66X59+/fl3dgb339+rU0v7q6Wt7h+PHjpfnJycnyDtvb26X5o0ePlnfYjZsTEEmc\ngEjiBEQSJyCSOAGRxAmIJE5AJHECIokTEEmcgEjiBEQSJyCSOAGRxAmIJE5AJHECInU+Nnf69OnS\n4Y8ePSrNt9ba9PR0af7kyZPlHS5evFiaHw6H5R1u3LhRPoN/VR9Ju3TpUnmHwWBQmn/48GF5h3fv\n3pXmT506Vd5hN25OQCRxAiKJExBJnIBI4gREEicgkjgBkcQJiCROQCRxAiKJExBJnIBI4gREEicg\nkjgBkTrfc3rz5k3p8M+fP5fmW2vt3LlzpfmdnZ3yDlNTU6X5EydOlHdgb1XfCZuZmSnvUD3jwYMH\n5R0uX75cmt/c3CzvsBs3JyCSOAGRxAmIJE5AJHECIokTEEmcgEjiBEQSJyCSOAGRxAmIJE5AJHEC\nIokTEEmcgEjiBETqfGzuyJEjpcMHg0FpvrXWtra2SvNLS0vlHa5du1aa//LlS3kH9tbISOen/1d7\n8YjhvXv3SvNnzpwp7zAxMVGa/y+/bTcnIJI4AZHECYgkTkAkcQIiiRMQSZyASOIERBInIJI4AZHE\nCYgkTkAkcQIiiRMQSZyASJ2P2nz//r10+OzsbGm+tdZev35dmr9w4UJ5h48fP5bmNzc3yzuwtzY2\nNkrzz549K+9w/fr10vytW7fKO/T7/dJ89e/Yxc0JiCROQCRxAiKJExBJnIBI4gREEicgkjgBkcQJ\niCROQCRxAiKJExBJnIBI4gREEicgkjgBkXrD4XC/dwD4jZsTEEmcgEjiBEQSJyCSOAGRxAmIJE5A\nJHECIokTEEmcgEjiBEQSJyCSOAGRxAmIJE5AJHECIokTEEmcgEjiBEQa6frx7t27pQfGjx07Vhlv\nrbU2OTlZmh8dHS3vsLa2Vprv9/vlHebm5nrlQ+B/xM0JiCROQCRxAiKJExBJnIBI4gREEicgkjgB\nkcQJiCROQCRxAiKJExBJnIBI4gREEicgUud7TisrK6XDX716VZpvrbWXL1+W5tfX18s7jIx0/pn+\n6sqVK+Ud4KBxcwIiiRMQSZyASOIERBInIJI4AZHECYgkTkAkcQIiiRMQSZyASOIERBInIJI4AZHE\nCYgkTkCkzlfUzp8/Xzq8Ot9aa+Pj46X5nz9/lnf48OFDaf7FixflHeCgcXMCIokTEEmcgEjiBEQS\nJyCSOAGRxAmIJE5AJHECIokTEEmcgEjiBEQSJyCSOAGRxAmI1PmeU7/fLx1++/bt0nxrra2srJTm\nnz59Wt7hzp07pfnV1dXyDnDQuDkBkcQJiCROQCRxAiKJExBJnIBI4gREEicgkjgBkcQJiCROQCRx\nAiKJExBJnIBI4gREEicgUudjcz9+/Cgd3uv1SvOttfbkyZPS/Pz8fHmH5eXl0vy3b9/KO8BB4+YE\nRBInIJI4AZHECYgkTkAkcQIiiRMQSZyASOIERBInIJI4AZHECYgkTkAkcQIiiRMQqfM9p42NjdLh\njx8/Ls231trz589L81evXi3v8Pbt29L87OxseQc4aNycgEjiBEQSJyCSOAGRxAmIJE5AJHECIokT\nEEmcgEjiBEQSJyCSOAGRxAmIJE5AJHECIokTEKk3HA73eweA37g5AZHECYgkTkAkcQIiiRMQSZyA\nSL8AtzvFjDEkZlcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFXvOGVz-1GH",
        "colab_type": "text"
      },
      "source": [
        "So the same thing as the second convolutional layer has 3 units it shows 3 outputs and the logic is same as the first one. Output of one layer  becomes the input of the other layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN5C3z0k_DOW",
        "colab_type": "text"
      },
      "source": [
        "Now showing the weights of the first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeH6mZ2N2R5W",
        "colab_type": "code",
        "outputId": "d2cff141-a44b-444b-c3cc-4b3dafb556a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "\n",
        "show_layer_weights(nnet, 0);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAABxCAYAAACX+mUfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAACJElEQVR4nO3asW0qQQBF0V2LgIySoAXog2KojZRg\nJUQHpGh/AR8TWZ4r65x0kheMriaYeV3XCaDma/QAgHfECUgSJyBJnIAkcQKSxAlI2nw6nOd5+D+D\n/X4/esK0LMvoCdOyLPPoDX9J4W6fz+fRE6bD4TB6wnQ6nd7ebS8nIEmcgCRxApLECUgSJyBJnIAk\ncQKSxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlIEicgSZyAJHECksQJ\nSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlI2nw6vFwuv7XjW6/Xa/SExAZ+1uPxGD1hut/voydM\nu91u9IRveTkBSeIEJIkTkCROQJI4AUniBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCRO\nQJI4AUniBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJE5AkTkDSvK7r\np/OPh7/hdruNnjBdr9fRE6bj8TiP3vDHDL/bz+dz9IRps9mMnjBtt9u3d9vLCUgSJyBJnIAkcQKS\nxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlIEicgSZyAJHECksQJSBIn\nIEmcgCRxApLECUgSJyBJnIAkcQKSxAlIEicgSZyAJHECkuZ1XUdvAPiPlxOQJE5AkjgBSeIEJIkT\nkCROQNI/hmgtAY+WLPgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKjt2wnZ_lKY",
        "colab_type": "text"
      },
      "source": [
        "So the weights from the first unit of the first convolutional layer is used to convolve the image and as a  result a lighter image is produced with a darer shade at the top corner according to the weights . And when that result is taken as input byu the second unit it applies its own weight and due to the presence of a black pixel it also produces a more dark black type thing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO3udB30_IhJ",
        "colab_type": "text"
      },
      "source": [
        "Showing the weights of the second layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osb5xVb62bEn",
        "colab_type": "code",
        "outputId": "1eef7638-0371-407c-8444-85adb113c79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "show_layer_weights(nnet, 1);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAADnCAYAAABcxZBBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAD/klEQVR4nO3cMUprURhG0ZNHGhEh4FS0sdJWgpUD\ncQ4WTsXKRmfhVLQStLxvAJpUj5yNb632NF9x2fwQyGpZlgFQ82f2AICfiBOQJE5AkjgBSeIEJK33\nPT4/P0//Ke/09HT2hHFxcTF7whhjrGYP+E3u7++nf9tnZ2ezJ4zr6+vZE8ayLD9+2y4nIEmcgCRx\nApLECUgSJyBJnIAkcQKSxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlI\nEicgSZyAJHECksQJSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlIWu97vLm5OdSOnbbb7ewJ4/j4\nePaE8fj4OHvCr/L5+Tl7QsLt7e3sCTu5nIAkcQKSxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLE\nCUgSJyBJnIAkcQKSxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlIEicg\nSZyAJHECksQJSBInIGm97/H8/PxQO3a6urqaPWGcnJzMnsA/9vDwMHvCeH9/nz1hbDab2RN2cjkB\nSeIEJIkTkCROQJI4AUniBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJ\nE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJE5AkTkDSalmW2RsAvnE5AUni\nBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJE5AkTkCSOAFJ4gQkiROQ\ntN73+PX1Nf0Pxo+OjmZPGC8vL7MnjO12u5q9AQ7J5QQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUni\nBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJE5AkTkCSOAFJ4gQkiROQ\nJE5AkjgBSeIEJIkTkCROQJI4AUnrfY9vb2+H2rHT09PT7Anj7u5u9oSxLMvsCXBQLicgSZyAJHEC\nksQJSBInIEmcgCRxApLECUgSJyBJnIAkcQKSxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLECUgS\nJyBJnIAkcQKSxAlIEicgSZyAJHECksQJSBInIEmcgCRxApLECUha73t8fX091I6dPj4+Zk8Yl5eX\nsyfAf8flBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJE5AkTkCSOAFJ\n4gQkiROQJE5AkjgBSeIEJIkTkCROQJI4AUniBCSJE5AkTkCSOAFJ4gQkiROQJE5AkjgBSatlWWZv\nAPjG5QQkiROQJE5AkjgBSeIEJIkTkPQXpfg5vthVkL0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpR5PZiZ2jl9",
        "colab_type": "text"
      },
      "source": [
        "Extra Credit 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvg5kXnf2z6L",
        "colab_type": "text"
      },
      "source": [
        "First with gpu=False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv0beBKy2lMk",
        "colab_type": "code",
        "outputId": "a79a9a7e-51b1-4550-d72d-dc83ee08d5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[5],\n",
        "                                   kernels_size_and_stride=[[3,2]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[2], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=False)\n",
        "\n",
        "Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "Xtrain1=torch.tensor(Xtrain1)\n",
        "Xtrain1=Xtrain1.float()\n",
        "nnet.train(Xtrain1, Ttrain, 200, learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3856313228607178\n",
            "Epoch 10 error 1.968096137046814\n",
            "Epoch 20 error 1.8511104583740234\n",
            "Epoch 30 error 1.7621294260025024\n",
            "Epoch 40 error 1.6827312707901\n",
            "Epoch 50 error 1.610481858253479\n",
            "Epoch 60 error 1.5470079183578491\n",
            "Epoch 70 error 1.4921107292175293\n",
            "Epoch 80 error 1.4486924409866333\n",
            "Epoch 90 error 1.4089754819869995\n",
            "Epoch 100 error 1.3777592182159424\n",
            "Epoch 110 error 1.3500617742538452\n",
            "Epoch 120 error 1.328441858291626\n",
            "Epoch 130 error 1.3077043294906616\n",
            "Epoch 140 error 1.2894625663757324\n",
            "Epoch 150 error 1.2748444080352783\n",
            "Epoch 160 error 1.2626150846481323\n",
            "Epoch 170 error 1.2525144815444946\n",
            "Epoch 180 error 1.2446321249008179\n",
            "Epoch 190 error 1.232884407043457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye66QYQw3P1r",
        "colab_type": "text"
      },
      "source": [
        "Training time for gpu=false"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APEVmWOv295a",
        "colab_type": "code",
        "outputId": "36d4fee3-390d-4d9c-8ea3-6cad2c8b60b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f'  Training took {nnet.training_time:.3f} seconds.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Training took 350.909 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c5m5Zq53Z2g",
        "colab_type": "text"
      },
      "source": [
        "Nw with gpu=true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRS2wSnd3b3F",
        "colab_type": "code",
        "outputId": "2c5ec8a5-820c-4939-e042-8e0215de79c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "nnet = NeuralNetwork_Convolutional(n_channels_in_image=1,\n",
        "                                   image_size=28,\n",
        "                                   n_units_in_conv_layers=[5],\n",
        "                                   kernels_size_and_stride=[[3,2]],# , 5],\n",
        "                                   n_units_in_fc_hidden_layers=[2], # 10, 10],\n",
        "                                   classes=P,\n",
        "                                   use_gpu=True)\n",
        "\n",
        "Xtrain1=make_images1(25000,Xtrain)\n",
        "\n",
        "Xtrain1=torch.tensor(Xtrain1).to('cuda')\n",
        "Xtrain1=Xtrain1.float()\n",
        "nnet.train(Xtrain1, Ttrain, 200, learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 error 2.3590621948242188\n",
            "Epoch 10 error 1.9480531215667725\n",
            "Epoch 20 error 1.815647840499878\n",
            "Epoch 30 error 1.7194641828536987\n",
            "Epoch 40 error 1.6438478231430054\n",
            "Epoch 50 error 1.5867663621902466\n",
            "Epoch 60 error 1.5411614179611206\n",
            "Epoch 70 error 1.5029754638671875\n",
            "Epoch 80 error 1.4691754579544067\n",
            "Epoch 90 error 1.4380563497543335\n",
            "Epoch 100 error 1.408936858177185\n",
            "Epoch 110 error 1.3811194896697998\n",
            "Epoch 120 error 1.3546298742294312\n",
            "Epoch 130 error 1.3296074867248535\n",
            "Epoch 140 error 1.3065170049667358\n",
            "Epoch 150 error 1.2857012748718262\n",
            "Epoch 160 error 1.2673454284667969\n",
            "Epoch 170 error 1.2621870040893555\n",
            "Epoch 180 error 1.24311363697052\n",
            "Epoch 190 error 1.230446457862854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gmHvqt93ltD",
        "colab_type": "text"
      },
      "source": [
        "Time for GPU=true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpCt_nq3ogZ",
        "colab_type": "code",
        "outputId": "53e0a997-70e5-4a32-f5d8-e64f1d22d9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f'  Training took {nnet.training_time:.3f} seconds.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Training took 22.598 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAiLuH1JizDP",
        "colab_type": "text"
      },
      "source": [
        "So we can see that without gpu traning time is 350.909 sec while that with gpu is 22.598 sec. So there is a huge time difference. This is due to the fact that gpu can make several computations in a parallel manner due to presence of huge number of cores in GPU while the cpu cannot. \n",
        "\n",
        "\n",
        "Again if the dataset size is very small  the result will be reversed as the tme of taking the values from gpu to cpu is going to be affecting the runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "got-KgZrjVh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}